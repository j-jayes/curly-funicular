"""SCB (Statistics Sweden) data ingestion module.

This module fetches income data from SCB's API using JSON-stat2 format.
Implements rate limiting (3 req/sec) and request chunking to stay within
SCB's limits (30 req/10 sec, 150K cells/request).
"""

import httpx
import pandas as pd
from pyjstat import pyjstat
from typing import Dict, List, Optional
from pathlib import Path
import time
import logging
import hashlib

logger = logging.getLogger(__name__)

# NUTS region code to name mapping (used in this SCB table)
NUTS_REGION_CODES = {
    "SE": "Sweden",
    "SE11": "Stockholm",
    "SE12": "East-Central Sweden",
    "SE21": "Småland and islands",
    "SE22": "South Sweden",
    "SE23": "West Sweden",
    "SE31": "North-Central Sweden",
    "SE32": "Central Norrland",
    "SE33": "Upper Norrland",
}

# All NUTS region codes for full coverage
ALL_REGION_CODES = list(NUTS_REGION_CODES.keys())

# Sector codes
SECTOR_CODES = {
    "0": "All sectors",
    "1-3": "Public sector",
    "4-5": "Private sector",
}

# Gender code mapping
GENDER_CODES = {"1": "men", "2": "women", "1+2": "total"}

# Contents code mapping  
CONTENTS_CODES = {
    "000007AQ": "basic_salary",
    "000007AS": "monthly_salary",
    "000007AR": "gender_salary_ratio",
    "000007AP": "num_employees",
}

# Salary dispersion contents codes (from LoneSpridSektYrk4AN)
DISPERSION_CONTENTS_CODES = {
    "000007CD": "monthly_salary",
    "000007CE": "median",
    "000007CF": "p10",  # 10th percentile
    "000007CG": "p25",  # 25th percentile (Q1)
    "000007CH": "p75",  # 75th percentile (Q3)
    "000007CI": "p90",  # 90th percentile
}

# Fallback codes if metadata fetch fails
DEFAULT_SSYK_CODES = ["2512", "2513", "2514"] 


class SCBIngestion:
    """Handles data ingestion from Statistics Sweden (SCB) API."""
    
    BASE_URL = "https://api.scb.se/OV0104/v1/doris/en/ssd"
    INCOME_ENDPOINT = "/AM/AM0110/AM0110A/LonYrkeRegion4AN"
    DISPERSION_ENDPOINT = "/AM/AM0110/AM0110A/LoneSpridSektYrk4AN"
    AGE_ENDPOINT = "/AM/AM0110/AM0110A/LonYrkeAlder4AN"
    EDUCATION_ENDPOINT = "/AM/AM0110/AM0110A/LonYrkeUtb4AN"
    
    # Rate limiting: 1 request per second (safe/slow as requested)
    MIN_REQUEST_INTERVAL = 1.0
    CHUNK_SIZE = 50 
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self._last_request_time = 0.0
        self.ssyk_mapping = {}
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
        }
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        
        self.client = httpx.Client(
            headers=headers,
            timeout=60.0,
        )

    def _rate_limit(self):
        """Enforce rate limiting between requests."""
        elapsed = time.time() - self._last_request_time
        if elapsed < self.MIN_REQUEST_INTERVAL:
            time.sleep(self.MIN_REQUEST_INTERVAL - elapsed)
        self._last_request_time = time.time()

    def _chunk_list(self, lst: List, chunk_size: int) -> List[List]:
        return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]

    def _fetch_metadata(self, endpoint: str) -> Dict:
        self._rate_limit()
        url = f"{self.BASE_URL}{endpoint}"
        logger.info(f"Fetching metadata from {url}")
        try:
            response = self.client.get(url)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Error fetching metadata from {url}: {e}")
            raise
        
    def _ensure_ssyk_mapping(self):
        if not self.ssyk_mapping:
            try:
                meta = self._fetch_metadata(self.INCOME_ENDPOINT)
                for var in meta.get("variables", []):
                    if var["code"] == "Yrke2012":
                        codes = var["values"]
                        labels = var["valueTexts"]
                        self.ssyk_mapping = dict(zip(codes, labels))
                        break
            except Exception as e:
                logger.error(f"Failed to fetch SSYK mapping: {e}")

    def get_all_ssyk_codes(self) -> List[str]:
        """Fetch all available SSYK 2012 codes from SCB metadata."""
        self._ensure_ssyk_mapping()
        if self.ssyk_mapping:
            return list(self.ssyk_mapping.keys())
        return DEFAULT_SSYK_CODES

    def get_table_metadata(self) -> Dict:
        return self._fetch_metadata(self.INCOME_ENDPOINT)
    
    def fetch_income_data(
        self, 
        occupation_codes: Optional[List[str]] = None,
        region_codes: Optional[List[str]] = None,
        years: Optional[List[str]] = None,
        genders: Optional[List[str]] = None,
        sectors: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        if occupation_codes is None:
            occupation_codes = self.get_all_ssyk_codes()
        if region_codes is None:
            region_codes = ALL_REGION_CODES
        if years is None:
            years = [str(y) for y in range(2014, 2026)]
        if genders is None:
            genders = ["1", "2"]
        if sectors is None:
            sectors = ["0"]
        
        chunks = self._chunk_list(occupation_codes, self.CHUNK_SIZE)
        all_dfs = []
        
        logger.info(f"Fetching regional income data for {len(occupation_codes)} occupations in {len(chunks)} chunks.")

        for i, chunk in enumerate(chunks):
            query = {
                "query": [
                    {"code": "Region", "selection": {"filter": "item", "values": region_codes}},
                    {"code": "Sektor", "selection": {"filter": "item", "values": sectors}},
                    {"code": "Yrke2012", "selection": {"filter": "item", "values": chunk}},
                    {"code": "Kon", "selection": {"filter": "item", "values": genders}},
                    {"code": "ContentsCode", "selection": {"filter": "item", "values": ["000007AS", "000007AP"]}},
                    {"code": "Tid", "selection": {"filter": "item", "values": years}}
                ],
                "response": {"format": "json-stat2"}
            }
            
            self._rate_limit()
            url = f"{self.BASE_URL}{self.INCOME_ENDPOINT}"
            
            try:
                response = self.client.post(url, json=query)
                response.raise_for_status()
                df = self._parse_jsonstat2(response.json())
                df = self._add_labels(df)
                all_dfs.append(df)
                logger.debug(f"Chunk {i+1} fetched {len(df)} rows")
            except Exception as e:
                logger.error(f"Error fetching chunk {i+1}: {e}")
                raise

        if not all_dfs:
            return pd.DataFrame()

        final_df = pd.concat(all_dfs, ignore_index=True)
        final_df["surrogate_key"] = final_df.apply(lambda row: self._generate_surrogate_key(row), axis=1)
        
        logger.info(f"Successfully fetched total {len(final_df)} regional income records")
        return final_df

    def fetch_salary_dispersion(
        self,
        occupation_codes: Optional[List[str]] = None,
        years: Optional[List[str]] = None,
        genders: Optional[List[str]] = None,
        sectors: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        if occupation_codes is None:
            occupation_codes = self.get_all_ssyk_codes()
        if years is None:
            years = [str(y) for y in range(2014, 2026)]
        if genders is None:
            genders = ["1", "2"]
        if sectors is None:
            sectors = ["0"]
        
        chunks = self._chunk_list(occupation_codes, self.CHUNK_SIZE)
        all_dfs = []
        
        logger.info(f"Fetching salary dispersion data for {len(occupation_codes)} occupations in {len(chunks)} chunks.")
        
        for i, chunk in enumerate(chunks):
            query = {
                "query": [
                    {"code": "Sektor", "selection": {"filter": "item", "values": sectors}},
                    {"code": "Yrke2012", "selection": {"filter": "item", "values": chunk}},
                    {"code": "Kon", "selection": {"filter": "item", "values": genders}},
                    {"code": "ContentsCode", "selection": {"filter": "item", "values": list(DISPERSION_CONTENTS_CODES.keys())}},
                    {"code": "Tid", "selection": {"filter": "item", "values": years}}
                ],
                "response": {"format": "json-stat2"}
            }
            
            self._rate_limit()
            url = f"{self.BASE_URL}{self.DISPERSION_ENDPOINT}"
            
            try:
                response = self.client.post(url, json=query)
                response.raise_for_status()
                df = self._parse_jsonstat2(response.json())
                df = self._add_dispersion_labels(df)
                all_dfs.append(df)
            except Exception as e:
                logger.error(f"Error fetching dispersion chunk {i+1}: {e}")
                raise
        
        if not all_dfs:
            return pd.DataFrame()
            
        combined_df = pd.concat(all_dfs, ignore_index=True)
        pivoted_df = self._pivot_dispersion_data(combined_df)
        pivoted_df["surrogate_key"] = pivoted_df.apply(lambda row: self._generate_surrogate_key(row), axis=1)
        
        logger.info(f"Successfully fetched {len(pivoted_df)} dispersion records")
        return pivoted_df

    def fetch_income_by_age(
        self,
        occupation_codes: Optional[List[str]] = None,
        years: Optional[List[str]] = None,
        genders: Optional[List[str]] = None,
        sectors: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        if occupation_codes is None:
            occupation_codes = self.get_all_ssyk_codes()
        if years is None:
            years = [str(y) for y in range(2014, 2026)]
        if genders is None:
            genders = ["1", "2"]
        if sectors is None:
            sectors = ["0"]

        try:
            meta = self._fetch_metadata(self.AGE_ENDPOINT)
            age_codes = []
            for var in meta.get("variables", []):
                if var["code"] == "Alder":
                    age_codes = var["values"]
                    break
        except Exception as e:
            logger.warning(f"Could not fetch Age metadata: {e}")
            age_codes = ["Tot"] 
        
        if not age_codes:
            return pd.DataFrame()

        chunks = self._chunk_list(occupation_codes, self.CHUNK_SIZE)
        all_dfs = []
        
        logger.info(f"Fetching income by age for {len(occupation_codes)} occupations in {len(chunks)} chunks.")

        for i, chunk in enumerate(chunks):
            query = {
                "query": [
                    {"code": "Sektor", "selection": {"filter": "item", "values": sectors}},
                    {"code": "Yrke2012", "selection": {"filter": "item", "values": chunk}},
                    {"code": "Kon", "selection": {"filter": "item", "values": genders}},
                    {"code": "Alder", "selection": {"filter": "item", "values": age_codes}},
                    {"code": "ContentsCode", "selection": {"filter": "item", "values": ["000007AS"]}},
                    {"code": "Tid", "selection": {"filter": "item", "values": years}}
                ],
                "response": {"format": "json-stat2"}
            }
            
            self._rate_limit()
            url = f"{self.BASE_URL}{self.AGE_ENDPOINT}"
            
            try:
                response = self.client.post(url, json=query)
                response.raise_for_status()
                df = self._parse_jsonstat2(response.json())
                df = self._add_labels(df)
                all_dfs.append(df)
            except Exception as e:
                logger.error(f"Error fetching age chunk {i+1}: {e}")
                raise

        if not all_dfs:
            return pd.DataFrame()

        final_df = pd.concat(all_dfs, ignore_index=True)
        final_df["surrogate_key"] = final_df.apply(lambda row: self._generate_surrogate_key(row), axis=1)
        logger.info(f"Successfully fetched {len(final_df)} age records")
        return final_df

    def fetch_income_by_education(
        self,
        occupation_codes: Optional[List[str]] = None,
        years: Optional[List[str]] = None,
        genders: Optional[List[str]] = None,
        sectors: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        if occupation_codes is None:
            occupation_codes = self.get_all_ssyk_codes()
        if years is None:
            years = [str(y) for y in range(2014, 2026)]
        if genders is None:
            genders = ["1", "2"]
        if sectors is None:
            sectors = ["0"]

        try:
            meta = self._fetch_metadata(self.EDUCATION_ENDPOINT)
            edu_codes = []
            for var in meta.get("variables", []):
                if var["code"] == "Utbildningsniva":
                    edu_codes = var["values"]
                    break
        except Exception as e:
            logger.warning(f"Could not fetch Education metadata: {e}")
            return pd.DataFrame()
        
        if not edu_codes:
            return pd.DataFrame()

        chunks = self._chunk_list(occupation_codes, self.CHUNK_SIZE)
        all_dfs = []
        
        logger.info(f"Fetching income by education for {len(occupation_codes)} occupations in {len(chunks)} chunks.")

        for i, chunk in enumerate(chunks):
            query = {
                "query": [
                    {"code": "Sektor", "selection": {"filter": "item", "values": sectors}},
                    {"code": "Yrke2012", "selection": {"filter": "item", "values": chunk}},
                    {"code": "Kon", "selection": {"filter": "item", "values": genders}},
                    {"code": "Utbildningsniva", "selection": {"filter": "item", "values": edu_codes}},
                    {"code": "ContentsCode", "selection": {"filter": "item", "values": ["000007AS"]}},
                    {"code": "Tid", "selection": {"filter": "item", "values": years}}
                ],
                "response": {"format": "json-stat2"}
            }

            self._rate_limit()
            url = f"{self.BASE_URL}{self.EDUCATION_ENDPOINT}"
            
            try:
                response = self.client.post(url, json=query)
                response.raise_for_status()
                df = self._parse_jsonstat2(response.json())
                df = self._add_labels(df)
                all_dfs.append(df)
            except Exception as e:
                logger.error(f"Error fetching education chunk {i+1}: {e}")
                raise

        if not all_dfs:
            return pd.DataFrame()

        final_df = pd.concat(all_dfs, ignore_index=True)
        final_df["surrogate_key"] = final_df.apply(lambda row: self._generate_surrogate_key(row), axis=1)
        logger.info(f"Successfully fetched {len(final_df)} education records")
        return final_df

    def _add_dispersion_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        df = self._add_labels(df)
        
        contents_col = None
        for col in ["ContentsCode", "contents"]:
            if col in df.columns:
                contents_col = col
                break
        
        if contents_col:
            df["measure"] = df[contents_col].map(DISPERSION_CONTENTS_CODES)
        
        return df
    
    def _pivot_dispersion_data(self, df: pd.DataFrame) -> pd.DataFrame:
        id_cols = []
        for col in ["Tid", "year", "Yrke2012", "occupation", "Kon", "sex", 
                    "Sektor", "sector", "gender", "occupation_name"]:
            if col in df.columns:
                id_cols.append(col)
        
        measure_col = "measure" if "measure" in df.columns else "ContentsCode"
        value_col = "value"
        
        if measure_col not in df.columns or value_col not in df.columns:
            logger.warning("Cannot pivot dispersion data - missing columns")
            return df
        
        try:
            unique_id_cols = list(set(id_cols) - {"measure", "ContentsCode", "contents"})
            pivot_df = df.pivot_table(
                index=unique_id_cols,
                columns=measure_col,
                values=value_col,
                aggfunc="first"
            ).reset_index()
            
            # Map SSYK codes if not already present
            self._ensure_ssyk_mapping()
            if "occupation_name" not in pivot_df.columns and self.ssyk_mapping:
                for col in ["Yrke2012", "occupation"]:
                    if col in pivot_df.columns:
                        pivot_df["occupation_name"] = pivot_df[col].map(self.ssyk_mapping)
                        break
            
            return pivot_df
        except Exception as e:
            logger.warning(f"Pivot failed: {e}, returning long format")
            return df
    
    def _parse_jsonstat2(self, data: Dict) -> pd.DataFrame:
        try:
            # Force using IDs instead of labels
            datasets = pyjstat.from_json_stat(data, naming='id')
            if datasets:
                df = datasets[0]
                if "value" in df.columns:
                    df["value"] = pd.to_numeric(df["value"], errors="coerce")
                return df
        except Exception as e:
            logger.warning(f"pyjstat parsing failed: {e}, trying manual parse")
        
        return self._manual_parse_jsonstat2(data)
    
    def _manual_parse_jsonstat2(self, data: Dict) -> pd.DataFrame:
        records = []
        dimensions = data.get("dimension", {})
        dim_ids = data.get("id", [])
        values = data.get("value", [])
        
        dim_values = {}
        for dim_id in dim_ids:
            dim_info = dimensions.get(dim_id, {})
            category = dim_info.get("category", {})
            index = category.get("index", {})
            label = category.get("label", {})
            
            if isinstance(index, dict):
                dim_values[dim_id] = sorted(index.keys(), key=lambda k: index[k])
            else:
                dim_values[dim_id] = list(label.keys()) if label else []
        
        from itertools import product
        all_combinations = list(product(*[dim_values[d] for d in dim_ids]))
        
        for i, combo in enumerate(all_combinations):
            if i < len(values):
                record = dict(zip(dim_ids, combo))
                record["value"] = values[i]
                records.append(record)
        
        df = pd.DataFrame(records)
        if "value" in df.columns:
            df["value"] = pd.to_numeric(df["value"], errors="coerce")
        
        return df
    
    def _add_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        # Map NUTS region codes
        region_col = None
        for col in ["Region", "region"]:
            if col in df.columns:
                region_col = col
                break
        
        if region_col:
            df["region_name"] = df[region_col].map(NUTS_REGION_CODES)
        
        # Map sector codes
        sector_col = None
        for col in ["Sektor", "sector"]:
            if col in df.columns:
                sector_col = col
                break
        
        if sector_col:
            df["sector_name"] = df[sector_col].map(SECTOR_CODES)
        
        # Map gender codes
        gender_col = None
        for col in ["Kon", "sex", "gender", "kön"]:
            if col in df.columns:
                gender_col = col
                break
        
        if gender_col:
            df["gender"] = df[gender_col].map(GENDER_CODES)
        
        # Map contents codes
        contents_col = None
        for col in ["ContentsCode", "contents"]:
            if col in df.columns:
                contents_col = col
                break
        
        if contents_col:
            df["measure"] = df[contents_col].map(CONTENTS_CODES)
        
        # Map SSYK codes
        self._ensure_ssyk_mapping()
        if self.ssyk_mapping:
             for col in ["Yrke2012", "occupation", "ssyk_code"]:
                if col in df.columns:
                    df["occupation_name"] = df[col].map(self.ssyk_mapping)
                    break

        return df
    
    def _generate_surrogate_key(self, row: pd.Series) -> str:
        key_parts = []
        for col in ["Tid", "Region", "Yrke2012", "Kon", "ContentsCode", 
                    "year", "region", "occupation", "sex", "contents",
                    "Alder", "age", "Utbildningsniva", "education_level"]:
            if col in row.index:
                key_parts.append(str(row[col]))
        
        key_string = "|".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()[:16]
    
    def save_to_parquet(self, df: pd.DataFrame, filepath: Path) -> Path:
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        df.to_parquet(filepath, index=False, engine="pyarrow")
        logger.info(f"Saved {len(df)} records to {filepath}")
        return filepath
    
    def save_to_gcs(self, df: pd.DataFrame, bucket_name: str, blob_name: str):
        from google.cloud import storage
        import tempfile
        import os
        
        logger.info(f"Saving data to GCS: gs://{bucket_name}/{blob_name}")
        
        with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as f:
            temp_file = f.name
        
        try:
            df.to_parquet(temp_file, index=False, engine="pyarrow")
            
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            blob = bucket.blob(blob_name)
            blob.upload_from_filename(temp_file)
            
            logger.info(f"Successfully saved to GCS")
        finally:
            os.unlink(temp_file)
    
    def close(self):
        self.client.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, *args):
        self.close()
